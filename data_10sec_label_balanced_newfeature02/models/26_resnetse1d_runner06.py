# models/26_resnetse1d_runner06.py
# ResNet1D + Squeeze-Excitation + Attention Pooling + Multi-Scale Stem
# Ø®Ø±ÙˆØ¬ÛŒ: logits Ø¨Ø§ Ø´Ú©Ù„ (B,1)
# CONFIG Ø¨Ø§ Ø§Ù¾ÙˆÚ©/Ø¨ØªÚ†/Ù„Ø±Ù†â€ŒØ±ÛŒØª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ùˆ dropout Ù†Ø³Ø¨ØªØ§Ù‹ Ù…Ø­Ø§ÙØ¸Ù‡â€ŒÚ©Ø§Ø±Ø§Ù†Ù‡

import torch
import torch.nn as nn
import torch.nn.functional as F

CONFIG = {"epochs": 30, "batch_size": 32, "lr": 1e-3, "patience": 8, "dropout": 0.25}

# --------- Building Blocks ---------
class SEBlock(nn.Module):
    """Squeeze-and-Excitation Ø±ÙˆÛŒ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ø± Ø²Ù…Ø§Ù†)."""
    def __init__(self, C, r=8):
        super().__init__()
        self.avg = nn.AdaptiveAvgPool1d(1)
        self.fc1 = nn.Linear(C, max(1, C // r))
        self.fc2 = nn.Linear(max(1, C // r), C)

    def forward(self, x):            # x: (B, C, T)
        s = self.avg(x).squeeze(-1)  # (B, C)
        s = F.relu(self.fc1(s), inplace=True)
        s = torch.sigmoid(self.fc2(s)).unsqueeze(-1)  # (B, C, 1)
        return x * s                 # channel-wise reweight

class ResBlock(nn.Module):
    """Basic Residual Block: Conv-BN-ReLU - Conv-BN + SE + skip."""
    def __init__(self, C):
        super().__init__()
        self.conv1 = nn.Conv1d(C, C, kernel_size=3, padding=1)
        self.bn1   = nn.BatchNorm1d(C)
        self.conv2 = nn.Conv1d(C, C, kernel_size=3, padding=1)
        self.bn2   = nn.BatchNorm1d(C)
        self.se    = SEBlock(C, r=8)

    def forward(self, x):
        identity = x
        out = self.conv1(x); out = self.bn1(out); out = F.relu(out, inplace=True)
        out = self.conv2(out); out = self.bn2(out)
        out = self.se(out)
        out = F.relu(out + identity, inplace=True)
        return out

class AttnPool1D(nn.Module):
    """Attention Pooling Ø±ÙˆÛŒ Ø¨Ø¹Ø¯ Ø²Ù…Ø§Ù†: ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ù†Ø±Ù… Ùˆ Ø¬Ù…Ø¹ ÙˆØ²Ù†ÛŒ."""
    def __init__(self, C):
        super().__init__()
        self.score = nn.Conv1d(C, 1, kernel_size=1)

    def forward(self, x):            # x: (B, C, T)
        a = self.score(x)            # (B, 1, T)
        w = torch.softmax(a, dim=-1) # (B, 1, T)
        z = (x * w).sum(dim=-1)      # (B, C)
        return z

# --------- Model ---------
class ResNetSE1D(nn.Module):
    """
    Stem Ú†Ù†Ø¯-Ù…Ù‚ÛŒØ§Ø³ (k=3/5/9) â†’ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ 1x1 â†’ Ø§Ø³ØªÛŒØ¬â€ŒÙ‡Ø§ (Residual + SE)
    Ø¨ÛŒÙ† Ø§Ø³ØªÛŒØ¬â€ŒÙ‡Ø§ MaxPool Ø¨Ø±Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒÚ©Ø±Ø¯Ù† T. Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ AttnPool Ùˆ Head.
    """
    def __init__(self, in_ch=16, drop_p=0.25):
        super().__init__()
        stem_out_each = 24  # ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ± Ù‡Ø± Ø´Ø§Ø®Ù‡â€ŒÛŒ Ù…ÙˆÙ„ØªÛŒâ€ŒØ§Ø³Ú©ÛŒÙ„
        self.stem3 = nn.Sequential(
            nn.Conv1d(in_ch, stem_out_each, kernel_size=3, padding=1),
            nn.BatchNorm1d(stem_out_each),
            nn.ReLU(inplace=True)
        )
        self.stem5 = nn.Sequential(
            nn.Conv1d(in_ch, stem_out_each, kernel_size=5, padding=2),
            nn.BatchNorm1d(stem_out_each),
            nn.ReLU(inplace=True)
        )
        self.stem9 = nn.Sequential(
            nn.Conv1d(in_ch, stem_out_each, kernel_size=9, padding=4),
            nn.BatchNorm1d(stem_out_each),
            nn.ReLU(inplace=True)
        )
        stem_total = stem_out_each * 3  # 72
        self.stem_proj = nn.Sequential(
            nn.Conv1d(stem_total, 64, kernel_size=1),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True)
        )

        # Stage 1: C=64, Ø¯Ùˆ Ø¨Ù„Ø§Ú©
        self.stage1 = nn.Sequential(ResBlock(64), ResBlock(64))
        self.pool1  = nn.MaxPool1d(2)   # 1500 -> 750

        # Stage 2: Ø§ÙØ²Ø§ÛŒØ´ Ú©Ø§Ù†Ø§Ù„ Ø¨Ù‡ 128 Ùˆ Ø¯Ùˆ Ø¨Ù„Ø§Ú©
        self.to128  = nn.Sequential(
            nn.Conv1d(64, 128, kernel_size=1),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True)
        )
        self.stage2 = nn.Sequential(ResBlock(128), ResBlock(128))
        self.pool2  = nn.MaxPool1d(2)   # 750 -> 375

        # Stage 3: C=128ØŒ ÛŒÚ© Ø¨Ù„Ø§Ú© + Dropout Ø³Ø¨Ú©
        self.stage3 = ResBlock(128)
        self.dropout = nn.Dropout(drop_p)

        # Attention pooling + Head
        self.attn = AttnPool1D(128)
        self.head = nn.Linear(128, 1)

        # init
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, nonlinearity="relu")
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity="relu")

    def forward(self, x):  # x: (B, C_in, T)
        s = torch.cat([self.stem3(x), self.stem5(x), self.stem9(x)], dim=1)
        s = self.stem_proj(s)

        s = self.stage1(s)
        s = self.pool1(s)

        s = self.to128(s)
        s = self.stage2(s)
        s = self.pool2(s)

        s = self.stage3(s)
        s = self.dropout(s)

        z = self.attn(s)          # (B, 128)
        logits = self.head(z)     # (B, 1)
        return logits

def build_model(input_channels=16, seq_len=1500):
    return ResNetSE1D(in_ch=input_channels, drop_p=CONFIG.get("dropout", 0.25))

# ---- Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ Ø¨Ø§ Ø±Ø§Ù†Ø± ----
if __name__ == "__main__":
    import os, sys, argparse
    ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    sys.path.insert(0, ROOT)

    # ğŸ‘‡ Ø§Ú¯Ø± Ù†Ø§Ù… Ø±Ø§Ù†Ø± ØªÙˆ ÙØ±Ù‚ Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…ÛŒÙ† import Ø±Ø§ ØªØºÛŒÛŒØ± Ø¨Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ runners.R06_runner_accfirst)
    try:
        import runners.R08_runner_acc_first as runner
    except Exception:
        # Ø¯Ø± Ø¨Ø¹Ø¶ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ Ø±Ø§Ù†Ø± Ú©Ù†Ø§Ø± Ø±ÛŒØ´Ù‡ Ø§Ø³Øª Ùˆ Ø¨Ø¯ÙˆÙ† Ù¾Ú©ÛŒØ¬ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯:
        import R08_runner_acc_first as runner

    p = argparse.ArgumentParser()
    p.add_argument("--epochs", type=int, default=None)
    p.add_argument("--batch_size", type=int, default=None)
    p.add_argument("--lr", type=float, default=None)
    p.add_argument("--patience", type=int, default=None)
    p.add_argument("--seed", type=int, default=42)
    args = p.parse_args()

    runner.run_with_model_path(
        __file__,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr,
        patience=args.patience,
        seed=args.seed
    )
